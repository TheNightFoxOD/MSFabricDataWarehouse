-- Fabric notebook source

-- METADATA ********************

-- META {
-- META   "kernel_info": {
-- META     "name": "synapse_pyspark"
-- META   },
-- META   "dependencies": {
-- META     "lakehouse": {
-- META       "default_lakehouse": "4aee8a32-be91-489f-89f3-1a819b188807",
-- META       "default_lakehouse_name": "Bronze",
-- META       "default_lakehouse_workspace_id": "b0f83c07-a701-49bb-a165-e06ca0ee4000",
-- META       "known_lakehouses": [
-- META         {
-- META           "id": "4aee8a32-be91-489f-89f3-1a819b188807"
-- META         }
-- META       ]
-- META     }
-- META   }
-- META }

-- CELL ********************

-- Create new schemas
CREATE SCHEMA IF NOT EXISTS metadata;
CREATE SCHEMA IF NOT EXISTS dataverse;
CREATE SCHEMA IF NOT EXISTS temp;

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

-- MAGIC %%sql
-- MAGIC -- 1. PipelineConfig: Master control table with purge tracking
-- MAGIC CREATE TABLE IF NOT EXISTS metadata.PipelineConfig (
-- MAGIC     TableId STRING NOT NULL,
-- MAGIC     TableName STRING NOT NULL,
-- MAGIC     SchemaName STRING NOT NULL,
-- MAGIC     PrimaryKeyColumn STRING NOT NULL,
-- MAGIC     SyncEnabled BOOLEAN NOT NULL,
-- MAGIC     TrackDeletes BOOLEAN NOT NULL,
-- MAGIC     LastPurgeDate TIMESTAMP,
-- MAGIC     PurgeRecordCount INT,
-- MAGIC     LastDailySync TIMESTAMP,
-- MAGIC     CreatedDate TIMESTAMP NOT NULL,
-- MAGIC     ModifiedDate TIMESTAMP NOT NULL
-- MAGIC ) USING DELTA

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

INSERT INTO PipelineConfig 
(TableId, TableName, SchemaName, PrimaryKeyColumn, SyncEnabled, TrackDeletes, CreatedDate, ModifiedDate)
SELECT 'a1b2c3d4-e5f6-7890-1234-567890abcdef', 'account', 'dataverse', 'accountid', true, true, current_timestamp(), current_timestamp()
UNION ALL
SELECT 'b2c3d4e5-f6g7-8901-2345-6789012bcdef', 'donation', 'dataverse', 'donationid', true, true, current_timestamp(), current_timestamp()
UNION ALL
SELECT 'c3d4e5f6-g7h8-9012-3456-78901234cdef', 'activitypointer', 'dataverse', 'activityid', true, true, current_timestamp(), current_timestamp();

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark",
-- META   "frozen": true,
-- META   "editable": false
-- META }

-- CELL ********************

-- MAGIC %%pyspark
-- MAGIC # Databricks notebook source
-- MAGIC ## Enhanced PipelineConfig Seeder from Link to Fabric Tables
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC import uuid
-- MAGIC from datetime import datetime
-- MAGIC from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Configuration
-- MAGIC 
-- MAGIC STAGING_LAKEHOUSE = "Dataverse_Staging"
-- MAGIC ENTITY_DEFINITIONS_TABLE = "Bronze.dbo.entity_definitions"
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Load Entity Definitions from Dataflow Table
-- MAGIC 
-- MAGIC def load_entity_definitions_from_table(table_name):
-- MAGIC     try:
-- MAGIC         entities_df = spark.table(table_name)
-- MAGIC         entities_df = entities_df.select("LogicalName", "PrimaryIdAttribute")
-- MAGIC         entities = [
-- MAGIC             {
-- MAGIC                 'LogicalName': row.LogicalName,
-- MAGIC                 'PrimaryIdAttribute': row.PrimaryIdAttribute
-- MAGIC             }
-- MAGIC             for row in entities_df.collect()
-- MAGIC             if row.LogicalName and row.PrimaryIdAttribute
-- MAGIC         ]
-- MAGIC         return entities
-- MAGIC     except Exception as e:
-- MAGIC         print(f"‚ùå Error loading from table {table_name}: {e}")
-- MAGIC         print("Make sure the Dataflow has been run at least once")
-- MAGIC         return []
-- MAGIC 
-- MAGIC print(f"üìä Loading entity definitions from: {ENTITY_DEFINITIONS_TABLE}")
-- MAGIC entities = load_entity_definitions_from_table(ENTITY_DEFINITIONS_TABLE)
-- MAGIC 
-- MAGIC if entities:
-- MAGIC     print(f"‚úÖ Loaded {len(entities)} entity definitions")
-- MAGIC     entity_lookup = {
-- MAGIC         entity['LogicalName']: entity['PrimaryIdAttribute'] 
-- MAGIC         for entity in entities
-- MAGIC     }
-- MAGIC     print(f"üìã Created lookup dictionary with {len(entity_lookup)} entities")
-- MAGIC else:
-- MAGIC     print("‚ùå Failed to load entity definitions")
-- MAGIC     entity_lookup = {}
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Get Staging Tables from Link to Fabric Lakehouse
-- MAGIC 
-- MAGIC def get_staging_tables(lakehouse_name):
-- MAGIC     try:
-- MAGIC         tables_df = spark.sql(f"SHOW TABLES IN {lakehouse_name}.dbo")
-- MAGIC         table_names = [row.tableName for row in tables_df.collect()]
-- MAGIC         print(f"‚úÖ Found {len(table_names)} tables in {lakehouse_name}.dbo")
-- MAGIC         return set(table_names)
-- MAGIC     except Exception as e:
-- MAGIC         print(f"‚ùå Error querying staging lakehouse: {e}")
-- MAGIC         return set()
-- MAGIC 
-- MAGIC staging_tables = get_staging_tables(STAGING_LAKEHOUSE)
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Match Tables and Generate PipelineConfig Records
-- MAGIC 
-- MAGIC def generate_pipeline_config_records(staging_tables, entity_lookup):
-- MAGIC     records = []
-- MAGIC     current_time = datetime.now()
-- MAGIC     matched_count = 0
-- MAGIC     missing_in_definitions = 0
-- MAGIC     
-- MAGIC     for table_name in staging_tables:
-- MAGIC         if table_name in entity_lookup:
-- MAGIC             primary_key = entity_lookup[table_name]
-- MAGIC             table_id = str(uuid.uuid4())
-- MAGIC             
-- MAGIC             record = {
-- MAGIC                 'TableId': table_id,
-- MAGIC                 'TableName': table_name,
-- MAGIC                 'SchemaName': 'dataverse',
-- MAGIC                 'PrimaryKeyColumn': primary_key,
-- MAGIC                 'SyncEnabled': True,
-- MAGIC                 'TrackDeletes': True,
-- MAGIC                 'LastPurgeDate': None,
-- MAGIC                 'PurgeRecordCount': None,
-- MAGIC                 'LastDailySync': None,
-- MAGIC                 'CreatedDate': current_time,
-- MAGIC                 'ModifiedDate': current_time
-- MAGIC             }
-- MAGIC             records.append(record)
-- MAGIC             matched_count += 1
-- MAGIC         else:
-- MAGIC             missing_in_definitions += 1
-- MAGIC             print(f"‚ö†Ô∏è  Table '{table_name}' in staging but not in entity definitions - skipping")
-- MAGIC     
-- MAGIC     print(f"\nüìä Matching Summary:")
-- MAGIC     print(f"   - Tables in staging: {len(staging_tables)}")
-- MAGIC     print(f"   - Entities in definitions: {len(entity_lookup)}")
-- MAGIC     print(f"   - ‚úÖ Matched (will seed): {matched_count}")
-- MAGIC     print(f"   - ‚ö†Ô∏è  In staging but not in definitions: {missing_in_definitions}")
-- MAGIC     
-- MAGIC     return records
-- MAGIC 
-- MAGIC if staging_tables and entity_lookup:
-- MAGIC     config_records = generate_pipeline_config_records(staging_tables, entity_lookup)
-- MAGIC     print(f"\n‚úÖ Generated {len(config_records)} records")
-- MAGIC else:
-- MAGIC     print("‚ùå Cannot generate records")
-- MAGIC     config_records = []
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Create DataFrame
-- MAGIC 
-- MAGIC pipeline_config_schema = StructType([
-- MAGIC     StructField("TableId", StringType(), False),
-- MAGIC     StructField("TableName", StringType(), False),
-- MAGIC     StructField("SchemaName", StringType(), False),
-- MAGIC     StructField("PrimaryKeyColumn", StringType(), False),
-- MAGIC     StructField("SyncEnabled", BooleanType(), False),
-- MAGIC     StructField("TrackDeletes", BooleanType(), False),
-- MAGIC     StructField("LastPurgeDate", TimestampType(), True),
-- MAGIC     StructField("PurgeRecordCount", IntegerType(), True),
-- MAGIC     StructField("LastDailySync", TimestampType(), True),
-- MAGIC     StructField("CreatedDate", TimestampType(), False),
-- MAGIC     StructField("ModifiedDate", TimestampType(), False)
-- MAGIC ])
-- MAGIC 
-- MAGIC if config_records:
-- MAGIC     df = spark.createDataFrame(config_records, schema=pipeline_config_schema)
-- MAGIC     print("‚úÖ DataFrame created")
-- MAGIC else:
-- MAGIC     df = None
-- MAGIC     print("‚ùå No data to create DataFrame")
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Check for Existing Records
-- MAGIC 
-- MAGIC def check_existing_tables():
-- MAGIC     try:
-- MAGIC         existing_df = spark.sql("SELECT TableName FROM Master_Bronze.metadata.pipelineconfig")
-- MAGIC         existing_tables = [row.TableName for row in existing_df.collect()]
-- MAGIC         return set(existing_tables)
-- MAGIC     except Exception as e:
-- MAGIC         print(f"‚ÑπÔ∏è  Table might be empty: {e}")
-- MAGIC         return set()
-- MAGIC 
-- MAGIC def filter_new_records(config_records, existing_tables):
-- MAGIC     new_records = []
-- MAGIC     duplicate_count = 0
-- MAGIC     
-- MAGIC     for record in config_records:
-- MAGIC         if record['TableName'] in existing_tables:
-- MAGIC             duplicate_count += 1
-- MAGIC         else:
-- MAGIC             new_records.append(record)
-- MAGIC     
-- MAGIC     print(f"\nüìä Duplicate Check:")
-- MAGIC     print(f"   - Total from matching: {len(config_records)}")
-- MAGIC     print(f"   - Already exist: {duplicate_count}")
-- MAGIC     print(f"   - üÜï New to insert: {len(new_records)}")
-- MAGIC     
-- MAGIC     return new_records
-- MAGIC 
-- MAGIC if config_records:
-- MAGIC     print("üîç Checking for existing records...")
-- MAGIC     existing_tables = check_existing_tables()
-- MAGIC     print(f"Found {len(existing_tables)} existing tables")
-- MAGIC     
-- MAGIC     new_records = filter_new_records(config_records, existing_tables)
-- MAGIC     
-- MAGIC     if new_records:
-- MAGIC         df_new = spark.createDataFrame(new_records, schema=pipeline_config_schema)
-- MAGIC         print(f"‚úÖ Prepared {len(new_records)} new records")
-- MAGIC     else:
-- MAGIC         df_new = None
-- MAGIC         print("‚ÑπÔ∏è  No new records to insert")
-- MAGIC else:
-- MAGIC     new_records = []
-- MAGIC     df_new = None
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Insert New Records
-- MAGIC 
-- MAGIC if df_new is not None and len(new_records) > 0:
-- MAGIC     try:
-- MAGIC         df_new.write \
-- MAGIC           .format("delta") \
-- MAGIC           .mode("append") \
-- MAGIC           .option("mergeSchema", "true") \
-- MAGIC           .saveAsTable("Master_Bronze.metadata.pipelineconfig")
-- MAGIC         
-- MAGIC         print(f"‚úÖ Successfully inserted {len(new_records)} records")
-- MAGIC         print(f"\nüéâ Seeding Complete!")
-- MAGIC         print(f"   - Staging tables scanned: {len(staging_tables)}")
-- MAGIC         print(f"   - Tables matched: {len(config_records)}")
-- MAGIC         print(f"   - New tables seeded: {len(new_records)}")
-- MAGIC         
-- MAGIC     except Exception as e:
-- MAGIC         print(f"‚ùå Error inserting records: {e}")
-- MAGIC else:
-- MAGIC     print("‚ÑπÔ∏è  No new records to insert")
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Verify Results
-- MAGIC 
-- MAGIC print("\nüîç Current PipelineConfig State:")
-- MAGIC current_config = spark.sql("""
-- MAGIC     SELECT TableName, SchemaName, PrimaryKeyColumn, SyncEnabled, CreatedDate
-- MAGIC     FROM Master_Bronze.metadata.pipelineconfig
-- MAGIC     ORDER BY TableName
-- MAGIC """)
-- MAGIC 
-- MAGIC print(f"\nTotal tables in PipelineConfig: {current_config.count()}")
-- MAGIC current_config.show(20, truncate=False)


-- METADATA ********************

-- META {
-- META   "language": "python",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

-- MAGIC %%pyspark
-- MAGIC ## DEPRECATED!
-- MAGIC 
-- MAGIC # Databricks notebook source
-- MAGIC ## PipelineConfig Seeder from Dataverse JSON
-- MAGIC # Populate pipelineconfig table from Dataverse EntityDefinitions JSON
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC import json
-- MAGIC import uuid
-- MAGIC from datetime import datetime
-- MAGIC from pyspark.sql import SparkSession
-- MAGIC from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType
-- MAGIC from pyspark.sql.functions import when, col, count as spark_count
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Step 1: Paste Your JSON Data Here
-- MAGIC # Copy what's needed from the JSON response of: https://yourorg.crm.dynamics.com/api/data/v9.2/EntityDefinitions?$select=LogicalName,PrimaryIdAttribute
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC # Paste your complete JSON response here
-- MAGIC json_data = """
-- MAGIC {
-- MAGIC     "@odata.context": "https://od-master.crm4.dynamics.com/api/data/v9.2/$metadata#EntityDefinitions(LogicalName,PrimaryIdAttribute)",
-- MAGIC     "value": [
-- MAGIC         {
-- MAGIC             "MetadataId": "70816501-edb9-4740-a16c-6a5efbc05d84",
-- MAGIC             "LogicalName": "account",
-- MAGIC             "PrimaryIdAttribute": "accountid"
-- MAGIC         }
-- MAGIC     ]
-- MAGIC }
-- MAGIC """
-- MAGIC 
-- MAGIC # Parse the JSON data
-- MAGIC try:
-- MAGIC     parsed_data = json.loads(json_data)
-- MAGIC     entities = parsed_data.get('value', [])
-- MAGIC     print(f"‚úÖ Successfully parsed JSON with {len(entities)} entities")
-- MAGIC     
-- MAGIC     # Show sample entities
-- MAGIC     print(f"Sample entities:")
-- MAGIC     for i, entity in enumerate(entities[:5]):
-- MAGIC         print(f"  {i+1}. {entity['LogicalName']} -> {entity['PrimaryIdAttribute']}")
-- MAGIC     if len(entities) > 5:
-- MAGIC         print(f"  ... and {len(entities) - 5} more entities")
-- MAGIC         
-- MAGIC except json.JSONDecodeError as e:
-- MAGIC     print(f"‚ùå Error parsing JSON: {e}")
-- MAGIC     entities = []
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Step 2: Generate PipelineConfig Records
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC def generate_pipeline_config_records(entities):
-- MAGIC     """
-- MAGIC     Generate PipelineConfig records from Dataverse entities
-- MAGIC     Schema based on: [Master_Bronze].[metadata].[pipelineconfig]
-- MAGIC     """
-- MAGIC     records = []
-- MAGIC     current_time = datetime.now()
-- MAGIC     
-- MAGIC     for entity in entities:
-- MAGIC         logical_name = entity.get('LogicalName', '')
-- MAGIC         primary_key = entity.get('PrimaryIdAttribute', '')
-- MAGIC         
-- MAGIC         # Skip if essential fields are missing
-- MAGIC         if not logical_name or not primary_key:
-- MAGIC             print(f"‚ö†Ô∏è  Skipping entity with missing data: {entity}")
-- MAGIC             continue
-- MAGIC         
-- MAGIC         # Generate TableId (GUID)
-- MAGIC         table_id = str(uuid.uuid4())
-- MAGIC         
-- MAGIC         # Create record matching the schema
-- MAGIC         record = {
-- MAGIC             'TableId': table_id,
-- MAGIC             'TableName': logical_name,
-- MAGIC             'SchemaName': 'dataverse',  # Standard for Dataverse
-- MAGIC             'PrimaryKeyColumn': primary_key,
-- MAGIC             'SyncEnabled': True,  # Enable all tables by default
-- MAGIC             'TrackDeletes': True,  # Enable delete tracking for all
-- MAGIC             'LastPurgeDate': None,  # Will be set after first purge
-- MAGIC             'PurgeRecordCount': None,  # Will be set after first purge
-- MAGIC             'LastDailySync': None,  # Will be set after first sync
-- MAGIC             'CreatedDate': current_time,
-- MAGIC             'ModifiedDate': current_time
-- MAGIC         }
-- MAGIC         
-- MAGIC         records.append(record)
-- MAGIC     
-- MAGIC     return records
-- MAGIC 
-- MAGIC # Generate the records
-- MAGIC if entities:
-- MAGIC     config_records = generate_pipeline_config_records(entities)
-- MAGIC     print(f"‚úÖ Parsed {len(config_records)} PipelineConfig records")
-- MAGIC     
-- MAGIC     # Show sample records
-- MAGIC     print(f"\nSample parsed records:")
-- MAGIC     for i, record in enumerate(config_records[:3]):
-- MAGIC         print(f"  {i+1}. {record['TableName']} -> {record['PrimaryKeyColumn']} (ID: {record['TableId'][:8]}...)")
-- MAGIC else:
-- MAGIC     config_records = []
-- MAGIC     print("‚ùå No entities to process")
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Step 3: Create DataFrame with Correct Schema
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC # Define schema to match the actual pipelineconfig table structure
-- MAGIC pipeline_config_schema = StructType([
-- MAGIC     StructField("TableId", StringType(), False),
-- MAGIC     StructField("TableName", StringType(), False),
-- MAGIC     StructField("SchemaName", StringType(), False),
-- MAGIC     StructField("PrimaryKeyColumn", StringType(), False),
-- MAGIC     StructField("SyncEnabled", BooleanType(), False),
-- MAGIC     StructField("TrackDeletes", BooleanType(), False),
-- MAGIC     StructField("LastPurgeDate", TimestampType(), True),
-- MAGIC     StructField("PurgeRecordCount", IntegerType(), True),
-- MAGIC     StructField("LastDailySync", TimestampType(), True),
-- MAGIC     StructField("CreatedDate", TimestampType(), False),
-- MAGIC     StructField("ModifiedDate", TimestampType(), False)
-- MAGIC ])
-- MAGIC 
-- MAGIC # Create DataFrame
-- MAGIC if config_records:
-- MAGIC     df = spark.createDataFrame(config_records, schema=pipeline_config_schema)
-- MAGIC     
-- MAGIC     print("‚úÖ DataFrame created successfully")
-- MAGIC     print(f"Schema validation:")
-- MAGIC     df.printSchema()
-- MAGIC     
-- MAGIC     print(f"\nSample data preview:")
-- MAGIC     df.select("TableName", "PrimaryKeyColumn", "SyncEnabled", "TrackDeletes", "CreatedDate").show(10, truncate=False)
-- MAGIC     
-- MAGIC else:
-- MAGIC     df = None
-- MAGIC     print("‚ùå No data to create DataFrame")
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Step 4: Check for Existing Records and Prevent Duplicates
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC def check_existing_tables():
-- MAGIC     """Check which tables already exist in pipelineconfig to prevent duplicates"""
-- MAGIC     try:
-- MAGIC         existing_df = spark.sql("SELECT TableName FROM Master_Bronze.metadata.pipelineconfig")
-- MAGIC         existing_tables = [row.TableName for row in existing_df.collect()]
-- MAGIC         return set(existing_tables)
-- MAGIC     except Exception as e:
-- MAGIC         print(f"‚ÑπÔ∏è  Could not read existing records (table might be empty): {e}")
-- MAGIC         return set()
-- MAGIC 
-- MAGIC def filter_new_records(config_records, existing_tables):
-- MAGIC     """Filter out records that already exist"""
-- MAGIC     new_records = []
-- MAGIC     duplicate_count = 0
-- MAGIC     
-- MAGIC     for record in config_records:
-- MAGIC         if record['TableName'] in existing_tables:
-- MAGIC             duplicate_count += 1
-- MAGIC             print(f"‚ö†Ô∏è  Skipping duplicate: {record['TableName']}")
-- MAGIC         else:
-- MAGIC             new_records.append(record)
-- MAGIC     
-- MAGIC     print(f"üìä Summary:")
-- MAGIC     print(f"   - Total entities from JSON: {len(config_records)}")
-- MAGIC     print(f"   - Already exist: {duplicate_count}")
-- MAGIC     print(f"   - New to insert: {len(new_records)}")
-- MAGIC     
-- MAGIC     return new_records
-- MAGIC 
-- MAGIC # Check for existing records
-- MAGIC if config_records:
-- MAGIC     print("üîç Checking for existing records...")
-- MAGIC     existing_tables = check_existing_tables()
-- MAGIC     print(f"Found {len(existing_tables)} existing tables in pipelineconfig")
-- MAGIC     
-- MAGIC     # Filter out duplicates
-- MAGIC     new_records = filter_new_records(config_records, existing_tables)
-- MAGIC     
-- MAGIC     if new_records:
-- MAGIC         # Create DataFrame with only new records
-- MAGIC         df_new = spark.createDataFrame(new_records, schema=pipeline_config_schema)
-- MAGIC         print(f"‚úÖ Prepared {len(new_records)} new records for insertion")
-- MAGIC     else:
-- MAGIC         df_new = None
-- MAGIC         print("‚ÑπÔ∏è  No new records to insert - all tables already exist")
-- MAGIC else:
-- MAGIC     new_records = []
-- MAGIC     df_new = None
-- MAGIC     print("‚ùå No records generated from JSON")
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC ## Step 5: Insert New Records into PipelineConfig Table
-- MAGIC 
-- MAGIC # COMMAND ----------
-- MAGIC 
-- MAGIC if df_new is not None and len(new_records) > 0:
-- MAGIC     try:
-- MAGIC         # Write to the pipelineconfig table
-- MAGIC         df_new.write \
-- MAGIC           .format("delta") \
-- MAGIC           .mode("append") \
-- MAGIC           .option("mergeSchema", "true") \
-- MAGIC           .saveAsTable("Master_Bronze.metadata.pipelineconfig")
-- MAGIC         
-- MAGIC         print(f"‚úÖ Successfully inserted {len(new_records)} new records into pipelineconfig table")
-- MAGIC         
-- MAGIC     except Exception as e:
-- MAGIC         print(f"‚ùå Error inserting records: {e}")
-- MAGIC         print("Trying alternative table reference...")
-- MAGIC         
-- MAGIC         try:
-- MAGIC             # Alternative approach using direct table name
-- MAGIC             df_new.write \
-- MAGIC               .format("delta") \
-- MAGIC               .mode("append") \
-- MAGIC               .option("mergeSchema", "true") \
-- MAGIC               .saveAsTable("pipelineconfig")
-- MAGIC             
-- MAGIC             print(f"‚úÖ Successfully inserted {len(new_records)} records using alternative method")
-- MAGIC             
-- MAGIC         except Exception as e2:
-- MAGIC             print(f"‚ùå Alternative method also failed: {e2}")
-- MAGIC             print("Please check your table reference and permissions")
-- MAGIC 
-- MAGIC else:
-- MAGIC     print("‚ÑπÔ∏è  No new records to insert")

-- METADATA ********************

-- META {
-- META   "language": "python",
-- META   "language_group": "synapse_pyspark",
-- META   "frozen": true,
-- META   "editable": false
-- META }

-- CELL ********************

select * from metadata.PipelineConfig
order by TableName

-- update metadata.PipelineConfig
-- set LastPurgeDate = NULL



-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

select count(*) from metadata.PipelineConfig

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark",
-- META   "frozen": false,
-- META   "editable": true
-- META }

-- CELL ********************

select count(*) from dbo.entity_definitions

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

-- MAGIC %%sql
-- MAGIC -- 2. SyncAuditLog: Execution tracking
-- MAGIC CREATE TABLE IF NOT EXISTS metadata.SyncAuditLog (
-- MAGIC     LogId STRING NOT NULL,
-- MAGIC     PipelineRunId STRING NOT NULL,
-- MAGIC     PipelineName STRING NOT NULL,
-- MAGIC     TableName STRING,
-- MAGIC     Operation STRING NOT NULL,
-- MAGIC     StartTime TIMESTAMP NOT NULL,
-- MAGIC     EndTime TIMESTAMP,
-- MAGIC     RowsProcessed INT,
-- MAGIC     RowsDeleted INT,
-- MAGIC     RowsPurged INT,
-- MAGIC     Status STRING NOT NULL,
-- MAGIC     ErrorMessage STRING,
-- MAGIC     Notes STRING,
-- MAGIC     RetryCount INT NOT NULL,
-- MAGIC     CreatedDate TIMESTAMP NOT NULL
-- MAGIC ) USING DELTA

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

-- MAGIC %%sql
-- MAGIC -- 3. CheckpointHistory: Fallback management
-- MAGIC -- CheckpointHistory: Recovery point tracking with multi-table coordination
-- MAGIC -- Supports both per-table checkpoints (for diagnostics) and batch checkpoints (for synchronized rollback)
-- MAGIC CREATE TABLE IF NOT EXISTS metadata.CheckpointHistory (
-- MAGIC     CheckpointId STRING NOT NULL,
-- MAGIC     CheckpointName STRING NOT NULL,
-- MAGIC     CheckpointType STRING NOT NULL,           -- 'Daily', 'DailyBatch', 'PrePurge', 'Manual', 'PreMaintenance', 'PostRollback'
-- MAGIC     CreatedDate TIMESTAMP NOT NULL,
-- MAGIC     TablesIncluded INT NOT NULL,              -- 1 for per-table, N for batch checkpoints
-- MAGIC     TotalRows BIGINT,
-- MAGIC     ValidationStatus STRING NOT NULL,         -- 'Validated', 'Failed', 'Pending'
-- MAGIC     RetentionDate DATE NOT NULL,
-- MAGIC     IsActive BOOLEAN NOT NULL,                -- Soft delete flag - never physically delete checkpoints
-- MAGIC     PipelineRunId STRING,                     -- Links all checkpoints from same pipeline execution
-- MAGIC     SchemaName STRING,                        -- e.g., 'bronze' (NULL for batch checkpoints)
-- MAGIC     TableName STRING                          -- e.g., 'account' (NULL for batch checkpoints)
-- MAGIC ) USING DELTA

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

-- MAGIC %%sql
-- MAGIC CREATE TABLE IF NOT EXISTS metadata.DataValidation (
-- MAGIC     ValidationId STRING NOT NULL,
-- MAGIC     ValidationDate TIMESTAMP NOT NULL,
-- MAGIC     TableName STRING NOT NULL,
-- MAGIC     SourceRowCount INT,
-- MAGIC     BronzeRowCount INT NOT NULL,
-- MAGIC     ActiveRowCount INT NOT NULL,
-- MAGIC     DeletedRowCount INT NOT NULL,
-- MAGIC     PurgedRowCount INT NOT NULL,
-- MAGIC     ValidationPassed BOOLEAN NOT NULL
-- MAGIC ) USING DELTA

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

ALTER TABLE metadata.DataValidation 
ADD COLUMNS (
    Notes STRING COMMENT 'JSON array of validation check details',
    ValidationContext STRING COMMENT 'Context: PostRollback, Daily, PrePurge, etc.'
);

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

OPTIMIZE [metadata].PipelineConfig;
OPTIMIZE [metadata].SyncAuditLog;
OPTIMIZE [metadata].CheckpointHistory;
OPTIMIZE [metadata].DataValidation;

-- METADATA ********************

-- META {
-- META   "language": "sparksql",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

-- MAGIC %%pyspark
-- MAGIC # ============================================================
-- MAGIC # CR-002: Enhanced Manual Rollback Capability
-- MAGIC # Setup Cell 1 of 2: Create Rollback Metadata Tables
-- MAGIC # ============================================================
-- MAGIC # 
-- MAGIC # PURPOSE: Creates metadata tables required for rollback functionality
-- MAGIC # RUN ONCE: Execute this cell, then run the optimization cell below
-- MAGIC # DEPENDENCIES: metadata schema must already exist (created in base setup)
-- MAGIC #
-- MAGIC # INSTRUCTIONS: 
-- MAGIC #   1. Run this cell first to create tables
-- MAGIC #   2. Wait for completion
-- MAGIC #   3. Then run the optimization cell (Cell 2)
-- MAGIC # ============================================================
-- MAGIC 
-- MAGIC print("="*60)
-- MAGIC print("Creating Rollback Metadata Tables")
-- MAGIC print("="*60)
-- MAGIC 
-- MAGIC # ============================================================
-- MAGIC # Step 1: Create RollbackStateSnapshots table
-- MAGIC # ============================================================
-- MAGIC print("\n1. Creating metadata.RollbackStateSnapshots...")
-- MAGIC 
-- MAGIC spark.sql("""
-- MAGIC     CREATE TABLE IF NOT EXISTS metadata.RollbackStateSnapshots (
-- MAGIC         SnapshotId STRING NOT NULL COMMENT 'Unique identifier for snapshot',
-- MAGIC         PipelineRunId STRING NOT NULL COMMENT 'Links to SyncAuditLog for traceability',
-- MAGIC         TableName STRING NOT NULL COMMENT 'Bronze table being captured',
-- MAGIC         SnapshotType STRING NOT NULL COMMENT 'PreRollback or PostRollback',
-- MAGIC         SnapshotDate TIMESTAMP NOT NULL COMMENT 'When snapshot was captured',
-- MAGIC         TotalRows BIGINT COMMENT 'Total records in table',
-- MAGIC         ActiveRows BIGINT COMMENT 'Records with IsDeleted=false and IsPurged=false',
-- MAGIC         DeletedRows BIGINT COMMENT 'Records with IsDeleted=true',
-- MAGIC         PurgedRows BIGINT COMMENT 'Records with IsPurged=true',
-- MAGIC         DeltaVersion BIGINT COMMENT 'Delta Lake version number at snapshot time',
-- MAGIC         SampleRecordIds STRING COMMENT 'JSON array of top 100 primary key values',
-- MAGIC         CreatedDate TIMESTAMP COMMENT 'Record creation timestamp'
-- MAGIC     ) USING DELTA
-- MAGIC     COMMENT 'Captures table state before and after rollback operations for validation'
-- MAGIC """)
-- MAGIC 
-- MAGIC print("   ‚úì RollbackStateSnapshots created")
-- MAGIC 
-- MAGIC # ============================================================
-- MAGIC # Step 2: Create RollbackComparisonReports table
-- MAGIC # ============================================================
-- MAGIC print("\n2. Creating metadata.RollbackComparisonReports...")
-- MAGIC 
-- MAGIC spark.sql("""
-- MAGIC     CREATE TABLE IF NOT EXISTS metadata.RollbackComparisonReports (
-- MAGIC         ReportId STRING NOT NULL COMMENT 'Unique identifier for report',
-- MAGIC         PipelineRunId STRING NOT NULL COMMENT 'Links to SyncAuditLog for traceability',
-- MAGIC         CheckpointId STRING NOT NULL COMMENT 'Foreign key to CheckpointHistory.CheckpointId',
-- MAGIC         CheckpointName STRING NOT NULL COMMENT 'Checkpoint name for readability',
-- MAGIC         RollbackDate TIMESTAMP NOT NULL COMMENT 'When rollback was executed',
-- MAGIC         TablesAffected INT COMMENT 'Number of tables included in rollback',
-- MAGIC         TablesSucceeded INT COMMENT 'Number of tables successfully restored',
-- MAGIC         TablesFailed INT COMMENT 'Number of tables that failed restoration',
-- MAGIC         RecordsRestored BIGINT COMMENT 'Total records added by rollback',
-- MAGIC         RecordsRemoved BIGINT COMMENT 'Total records removed by rollback',
-- MAGIC         QualityScore DECIMAL(5,2) COMMENT 'Validation quality score 0-100%',
-- MAGIC         ValidationPassed BOOLEAN COMMENT 'Overall validation pass/fail',
-- MAGIC         Issues STRING COMMENT 'JSON array of validation failures if any',
-- MAGIC         Notes STRING COMMENT 'Additional context or administrator notes',
-- MAGIC         CreatedDate TIMESTAMP COMMENT 'Record creation timestamp'
-- MAGIC     ) USING DELTA
-- MAGIC     COMMENT 'Comprehensive rollback analysis and validation results'
-- MAGIC """)
-- MAGIC 
-- MAGIC print("   ‚úì RollbackComparisonReports created")
-- MAGIC 
-- MAGIC # ============================================================
-- MAGIC # Step 3: Verify tables were created successfully
-- MAGIC # ============================================================
-- MAGIC print("\n3. Verifying table creation...")
-- MAGIC 
-- MAGIC tables = spark.sql("SHOW TABLES IN metadata").collect()
-- MAGIC table_names = [row.tableName for row in tables]
-- MAGIC 
-- MAGIC if 'rollbackstatesnapshots' in table_names:
-- MAGIC     print("   ‚úì RollbackStateSnapshots verified")
-- MAGIC else:
-- MAGIC     print("   ‚úó RollbackStateSnapshots NOT found!")
-- MAGIC 
-- MAGIC if 'rollbackcomparisonreports' in table_names:
-- MAGIC     print("   ‚úì RollbackComparisonReports verified")
-- MAGIC else:
-- MAGIC     print("   ‚úó RollbackComparisonReports NOT found!")
-- MAGIC 
-- MAGIC # ============================================================
-- MAGIC # Step 4: Display table schemas (optional - may need refresh)
-- MAGIC # ============================================================
-- MAGIC print("\n4. Verifying table schemas...")
-- MAGIC 
-- MAGIC try:
-- MAGIC     print("\n--- RollbackStateSnapshots Schema ---")
-- MAGIC     spark.sql("DESCRIBE metadata.RollbackStateSnapshots").show(truncate=False)
-- MAGIC except Exception as e:
-- MAGIC     print("   ‚ö†Ô∏è Could not describe RollbackStateSnapshots (table metadata still initializing)")
-- MAGIC     print("   ‚Üí This is normal. Refresh the lakehouse or wait a moment and try again.")
-- MAGIC 
-- MAGIC try:
-- MAGIC     print("\n--- RollbackComparisonReports Schema ---")
-- MAGIC     spark.sql("DESCRIBE metadata.RollbackComparisonReports").show(truncate=False)
-- MAGIC except Exception as e:
-- MAGIC     print("   ‚ö†Ô∏è Could not describe RollbackComparisonReports (table metadata still initializing)")
-- MAGIC     print("   ‚Üí This is normal. Refresh the lakehouse or wait a moment and try again.")
-- MAGIC 
-- MAGIC print("\n" + "="*60)
-- MAGIC print("‚úì Table Creation Complete!")
-- MAGIC print("="*60)
-- MAGIC print("\n‚úÖ Tables successfully created:")
-- MAGIC print("   ‚Ä¢ metadata.RollbackStateSnapshots")
-- MAGIC print("   ‚Ä¢ metadata.RollbackComparisonReports")
-- MAGIC print("\n‚ö†Ô∏è If DESCRIBE failed above, refresh your lakehouse in the left panel.")
-- MAGIC print("\n‚ö†Ô∏è NEXT STEP: Run the optimization cell below")
-- MAGIC print("="*60)

-- METADATA ********************

-- META {
-- META   "language": "python",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

-- MAGIC %%pyspark
-- MAGIC spark.sql("""
-- MAGIC     CREATE TABLE IF NOT EXISTS metadata.RollbackStateSnapshots (
-- MAGIC         SnapshotId STRING NOT NULL COMMENT 'Unique identifier for snapshot',
-- MAGIC         PipelineRunId STRING NOT NULL COMMENT 'Links to SyncAuditLog for traceability',
-- MAGIC         TableName STRING NOT NULL COMMENT 'Bronze table being captured',
-- MAGIC         SnapshotType STRING NOT NULL COMMENT 'PreRollback or PostRollback',
-- MAGIC         SnapshotDate TIMESTAMP NOT NULL COMMENT 'When snapshot was captured',
-- MAGIC         TotalRows BIGINT COMMENT 'Total records in table',
-- MAGIC         ActiveRows BIGINT COMMENT 'Records with IsDeleted=false and IsPurged=false',
-- MAGIC         DeletedRows BIGINT COMMENT 'Records with IsDeleted=true',
-- MAGIC         PurgedRows BIGINT COMMENT 'Records with IsPurged=true',
-- MAGIC         DeltaVersion BIGINT COMMENT 'Delta Lake version number at snapshot time',
-- MAGIC         SampleRecordIds STRING COMMENT 'JSON array of top 100 primary key values',
-- MAGIC         CreatedDate TIMESTAMP COMMENT 'Record creation timestamp'
-- MAGIC     ) USING DELTA
-- MAGIC     COMMENT 'Captures table state before and after rollback operations for validation'
-- MAGIC """)

-- METADATA ********************

-- META {
-- META   "language": "python",
-- META   "language_group": "synapse_pyspark"
-- META }

-- CELL ********************

-- MAGIC %%pyspark
-- MAGIC # ============================================================
-- MAGIC # CR-002: Enhanced Manual Rollback Capability
-- MAGIC # Setup Cell 2 of 2: Optimize Rollback Metadata Tables
-- MAGIC # ============================================================
-- MAGIC # 
-- MAGIC # PURPOSE: Optimizes the newly created rollback tables
-- MAGIC # PREREQUISITES: Cell 1 must have completed successfully
-- MAGIC #
-- MAGIC # INSTRUCTIONS: Run this cell AFTER tables have been created in Cell 1
-- MAGIC #
-- MAGIC # NOTE: If tables are empty, OPTIMIZE may not be necessary.
-- MAGIC #       This is optional and can be run later after tables have data.
-- MAGIC # ============================================================
-- MAGIC 
-- MAGIC print("="*60)
-- MAGIC print("Optimizing Rollback Metadata Tables")
-- MAGIC print("="*60)
-- MAGIC 
-- MAGIC # ============================================================
-- MAGIC # Step 1: Optimize RollbackStateSnapshots
-- MAGIC # ============================================================
-- MAGIC print("\n1. Optimizing RollbackStateSnapshots...")
-- MAGIC 
-- MAGIC try:
-- MAGIC     spark.sql("OPTIMIZE metadata.RollbackStateSnapshots")
-- MAGIC     print("   ‚úì RollbackStateSnapshots optimized")
-- MAGIC except Exception as e:
-- MAGIC     print(f"   ‚ö†Ô∏è Could not optimize (table may be empty): {str(e)}")
-- MAGIC     print("   ‚Üí This is normal for new tables. OPTIMIZE will work once data is added.")
-- MAGIC 
-- MAGIC # ============================================================
-- MAGIC # Step 2: Optimize RollbackComparisonReports
-- MAGIC # ============================================================
-- MAGIC print("\n2. Optimizing RollbackComparisonReports...")
-- MAGIC 
-- MAGIC try:
-- MAGIC     spark.sql("OPTIMIZE metadata.RollbackComparisonReports")
-- MAGIC     print("   ‚úì RollbackComparisonReports optimized")
-- MAGIC except Exception as e:
-- MAGIC     print(f"   ‚ö†Ô∏è Could not optimize (table may be empty): {str(e)}")
-- MAGIC     print("   ‚Üí This is normal for new tables. OPTIMIZE will work once data is added.")
-- MAGIC 
-- MAGIC # ============================================================
-- MAGIC # Step 3: Create Z-Order indexes
-- MAGIC # ============================================================
-- MAGIC print("\n3. Creating Z-Order indexes...")
-- MAGIC 
-- MAGIC try:
-- MAGIC     spark.sql("""
-- MAGIC         OPTIMIZE metadata.RollbackStateSnapshots 
-- MAGIC         ZORDER BY (PipelineRunId, TableName, SnapshotType, SnapshotDate)
-- MAGIC     """)
-- MAGIC     print("   ‚úì RollbackStateSnapshots Z-Order index created")
-- MAGIC except Exception as e:
-- MAGIC     print(f"   ‚ö†Ô∏è Could not create Z-Order index (table may be empty): {str(e)}")
-- MAGIC     print("   ‚Üí This is normal for new tables. Z-Order will work once data is added.")
-- MAGIC 
-- MAGIC try:
-- MAGIC     spark.sql("""
-- MAGIC         OPTIMIZE metadata.RollbackComparisonReports 
-- MAGIC         ZORDER BY (PipelineRunId, CheckpointId, RollbackDate, ValidationPassed)
-- MAGIC     """)
-- MAGIC     print("   ‚úì RollbackComparisonReports Z-Order index created")
-- MAGIC except Exception as e:
-- MAGIC     print(f"   ‚ö†Ô∏è Could not create Z-Order index (table may be empty): {str(e)}")
-- MAGIC     print("   ‚Üí This is normal for new tables. Z-Order will work once data is added.")
-- MAGIC 
-- MAGIC # ============================================================
-- MAGIC # Step 4: Verify final state
-- MAGIC # ============================================================
-- MAGIC print("\n4. Final verification:")
-- MAGIC 
-- MAGIC snapshots_count = spark.sql("SELECT COUNT(*) as count FROM metadata.RollbackStateSnapshots").collect()[0].count
-- MAGIC reports_count = spark.sql("SELECT COUNT(*) as count FROM metadata.RollbackComparisonReports").collect()[0].count
-- MAGIC 
-- MAGIC print(f"   RollbackStateSnapshots: {snapshots_count} rows")
-- MAGIC print(f"   RollbackComparisonReports: {reports_count} rows")
-- MAGIC 
-- MAGIC print("\n" + "="*60)
-- MAGIC print("‚úì Optimization Complete!")
-- MAGIC print("="*60)
-- MAGIC print("\nNext Steps:")
-- MAGIC print("1. Create the three rollback notebooks")
-- MAGIC print("2. Create the Manual Rollback to Checkpoint pipeline")
-- MAGIC print("3. Test with a dry-run rollback")
-- MAGIC print("="*60)

-- METADATA ********************

-- META {
-- META   "language": "python",
-- META   "language_group": "synapse_pyspark"
-- META }
